{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 11\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheraz\\AppData\\Local\\Temp\\ipykernel_27744\\1243690584.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained SimCLR model for TPC-RP selection.\n",
      "Number of typical points selected (budget) = 10\n",
      "Labeled samples: 10; Unlabeled samples: 49990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training on labeled data (400k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Stage 2: Generating pseudo-labels for unlabeled data...\n",
      "Stage 3: Fine-tuning on combined data (100k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Experiment with seed 11 => Accuracy: 13.53%\n",
      "Random seed set to 22\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheraz\\AppData\\Local\\Temp\\ipykernel_27744\\1243690584.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained SimCLR model for TPC-RP selection.\n",
      "Number of typical points selected (budget) = 10\n",
      "Labeled samples: 10; Unlabeled samples: 49990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training on labeled data (400k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Stage 2: Generating pseudo-labels for unlabeled data...\n",
      "Stage 3: Fine-tuning on combined data (100k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Experiment with seed 22 => Accuracy: 13.53%\n",
      "Random seed set to 33\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheraz\\AppData\\Local\\Temp\\ipykernel_27744\\1243690584.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained SimCLR model for TPC-RP selection.\n",
      "Number of typical points selected (budget) = 10\n",
      "Labeled samples: 10; Unlabeled samples: 49990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sheraz\\anaconda3\\envs\\ml_coursework2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training on labeled data (400k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Stage 2: Generating pseudo-labels for unlabeled data...\n",
      "Stage 3: Fine-tuning on combined data (100k iters)...\n",
      "[Semi-Sup Training] Progress: 1%\n",
      "[Semi-Sup Training] Progress: 2%\n",
      "[Semi-Sup Training] Progress: 3%\n",
      "[Semi-Sup Training] Progress: 4%\n",
      "[Semi-Sup Training] Progress: 5%\n",
      "[Semi-Sup Training] Progress: 6%\n",
      "[Semi-Sup Training] Progress: 7%\n",
      "[Semi-Sup Training] Progress: 8%\n",
      "[Semi-Sup Training] Progress: 9%\n",
      "[Semi-Sup Training] Progress: 10%\n",
      "[Semi-Sup Training] Progress: 11%\n",
      "[Semi-Sup Training] Progress: 12%\n",
      "[Semi-Sup Training] Progress: 13%\n",
      "[Semi-Sup Training] Progress: 14%\n",
      "[Semi-Sup Training] Progress: 15%\n",
      "[Semi-Sup Training] Progress: 16%\n",
      "[Semi-Sup Training] Progress: 17%\n",
      "[Semi-Sup Training] Progress: 18%\n",
      "[Semi-Sup Training] Progress: 19%\n",
      "[Semi-Sup Training] Progress: 20%\n",
      "[Semi-Sup Training] Progress: 21%\n",
      "[Semi-Sup Training] Progress: 22%\n",
      "[Semi-Sup Training] Progress: 23%\n",
      "[Semi-Sup Training] Progress: 24%\n",
      "[Semi-Sup Training] Progress: 25%\n",
      "[Semi-Sup Training] Progress: 26%\n",
      "[Semi-Sup Training] Progress: 27%\n",
      "[Semi-Sup Training] Progress: 28%\n",
      "[Semi-Sup Training] Progress: 29%\n",
      "[Semi-Sup Training] Progress: 30%\n",
      "[Semi-Sup Training] Progress: 31%\n",
      "[Semi-Sup Training] Progress: 32%\n",
      "[Semi-Sup Training] Progress: 33%\n",
      "[Semi-Sup Training] Progress: 34%\n",
      "[Semi-Sup Training] Progress: 35%\n",
      "[Semi-Sup Training] Progress: 36%\n",
      "[Semi-Sup Training] Progress: 37%\n",
      "[Semi-Sup Training] Progress: 38%\n",
      "[Semi-Sup Training] Progress: 39%\n",
      "[Semi-Sup Training] Progress: 40%\n",
      "[Semi-Sup Training] Progress: 41%\n",
      "[Semi-Sup Training] Progress: 42%\n",
      "[Semi-Sup Training] Progress: 43%\n",
      "[Semi-Sup Training] Progress: 44%\n",
      "[Semi-Sup Training] Progress: 45%\n",
      "[Semi-Sup Training] Progress: 46%\n",
      "[Semi-Sup Training] Progress: 47%\n",
      "[Semi-Sup Training] Progress: 48%\n",
      "[Semi-Sup Training] Progress: 49%\n",
      "[Semi-Sup Training] Progress: 50%\n",
      "[Semi-Sup Training] Progress: 51%\n",
      "[Semi-Sup Training] Progress: 52%\n",
      "[Semi-Sup Training] Progress: 53%\n",
      "[Semi-Sup Training] Progress: 54%\n",
      "[Semi-Sup Training] Progress: 55%\n",
      "[Semi-Sup Training] Progress: 56%\n",
      "[Semi-Sup Training] Progress: 57%\n",
      "[Semi-Sup Training] Progress: 58%\n",
      "[Semi-Sup Training] Progress: 59%\n",
      "[Semi-Sup Training] Progress: 60%\n",
      "[Semi-Sup Training] Progress: 61%\n",
      "[Semi-Sup Training] Progress: 62%\n",
      "[Semi-Sup Training] Progress: 63%\n",
      "[Semi-Sup Training] Progress: 64%\n",
      "[Semi-Sup Training] Progress: 65%\n",
      "[Semi-Sup Training] Progress: 66%\n",
      "[Semi-Sup Training] Progress: 67%\n",
      "[Semi-Sup Training] Progress: 68%\n",
      "[Semi-Sup Training] Progress: 69%\n",
      "[Semi-Sup Training] Progress: 70%\n",
      "[Semi-Sup Training] Progress: 71%\n",
      "[Semi-Sup Training] Progress: 72%\n",
      "[Semi-Sup Training] Progress: 73%\n",
      "[Semi-Sup Training] Progress: 74%\n",
      "[Semi-Sup Training] Progress: 75%\n",
      "[Semi-Sup Training] Progress: 76%\n",
      "[Semi-Sup Training] Progress: 77%\n",
      "[Semi-Sup Training] Progress: 78%\n",
      "[Semi-Sup Training] Progress: 79%\n",
      "[Semi-Sup Training] Progress: 80%\n",
      "[Semi-Sup Training] Progress: 81%\n",
      "[Semi-Sup Training] Progress: 82%\n",
      "[Semi-Sup Training] Progress: 83%\n",
      "[Semi-Sup Training] Progress: 84%\n",
      "[Semi-Sup Training] Progress: 85%\n",
      "[Semi-Sup Training] Progress: 86%\n",
      "[Semi-Sup Training] Progress: 87%\n",
      "[Semi-Sup Training] Progress: 88%\n",
      "[Semi-Sup Training] Progress: 89%\n",
      "[Semi-Sup Training] Progress: 90%\n",
      "[Semi-Sup Training] Progress: 91%\n",
      "[Semi-Sup Training] Progress: 92%\n",
      "[Semi-Sup Training] Progress: 93%\n",
      "[Semi-Sup Training] Progress: 94%\n",
      "[Semi-Sup Training] Progress: 95%\n",
      "[Semi-Sup Training] Progress: 96%\n",
      "[Semi-Sup Training] Progress: 97%\n",
      "[Semi-Sup Training] Progress: 98%\n",
      "[Semi-Sup Training] Progress: 99%\n",
      "[Semi-Sup Training] Progress: 100%\n",
      "Experiment with seed 33 => Accuracy: 14.72%\n",
      "\n",
      "After 3 runs:\n",
      "Mean Accuracy: 13.93% (+/- 0.56%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Import existing modules \n",
    "from typiclust_alg import SimCLRResNet18, compute_embeddings, typical_clustering_selection, DEVICE\n",
    "from visualisation import plot_tsne, set_seed\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided dataloader.\n",
    "    Returns the accuracy as a float.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_cifar10_datasets():\n",
    "    \"\"\"\n",
    "    Loads CIFAR-10 training and test datasets.\n",
    "    \"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "def build_wide_resnet():\n",
    "    \"\"\"\n",
    "    Builds a WideResNet model.\n",
    "    The paper uses WideResNet-28 for CIFAR-10.\n",
    "    Here, I use torchvision's wide_resnet50_2 as a proxy.\n",
    "    \"\"\"\n",
    "    model = torchvision.models.wide_resnet50_2(pretrained=False)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 10)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def train_model_iterations(model, dataloader, total_iterations, lr, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Trains the model for a total number of iterations (400,000)\n",
    "    Uses SGD with the specified learning rate and a cosine annealing scheduler\n",
    "    with T_max = total_iterations.\n",
    "\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005,\n",
    "        nesterov=True\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_iterations)\n",
    "\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    # For the progress bar display\n",
    "    last_shown_progress = 0\n",
    "\n",
    "    while iteration < total_iterations:\n",
    "        for images, labels in dataloader:\n",
    "            if iteration >= total_iterations:\n",
    "                break\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            iteration += 1\n",
    "\n",
    "            current_progress = int(100 * iteration / total_iterations)\n",
    "            if current_progress - last_shown_progress >= 1:\n",
    "                print(f\"[Semi-Sup Training] Progress: {current_progress}%\")\n",
    "                last_shown_progress = current_progress\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def select_samples_typiclust(dataset, budget, encoder):\n",
    "    \"\"\"\n",
    "    Uses the pre-loaded SimCLR encoder provided from the Forums to compute embeddings and select samples\n",
    "    \"\"\"\n",
    "    all_embeddings, _ = compute_embeddings(encoder, dataset, batch_size=128, num_workers=4)\n",
    "    all_labels = np.array([label for (_, label) in dataset])\n",
    "\n",
    "    selected_indices, cluster_labels = typical_clustering_selection(\n",
    "        all_embeddings,\n",
    "        budget=budget,\n",
    "        k_nn=20,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Number of typical points selected (budget) = {len(selected_indices)}\")\n",
    "    return selected_indices\n",
    "\n",
    "def generate_pseudo_labels(model, unlabeled_subset, batch_size=64, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Generate pseudo-labels for each image in 'unlabeled_subset'\n",
    "    \"\"\"\n",
    "    loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            pseudo_labels.extend(preds.cpu().numpy().tolist())\n",
    "    return pseudo_labels\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Combines:\n",
    "      - Labeled data from 'labeled_indices'\n",
    "      - Unlabeled data from 'unlabeled_indices' with pseudo_labels\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, labeled_indices, unlabeled_indices, pseudo_labels):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.combined_indices = labeled_indices + unlabeled_indices\n",
    "        self.label_map = {}\n",
    "\n",
    "        # Real labels for labeled indices\n",
    "        for idx in labeled_indices:\n",
    "            _, real_label = self.base_dataset[idx]\n",
    "            self.label_map[idx] = real_label\n",
    "\n",
    "        # Pseudo-labels for unlabeled indices\n",
    "        if len(unlabeled_indices) != len(pseudo_labels):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch: {len(unlabeled_indices)} unlabeled indices but \"\n",
    "                f\"{len(pseudo_labels)} pseudo-labels.\"\n",
    "            )\n",
    "        for i, u_idx in enumerate(unlabeled_indices):\n",
    "            self.label_map[u_idx] = pseudo_labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combined_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        real_idx = self.combined_indices[index]\n",
    "        img, _ = self.base_dataset[real_idx]\n",
    "        label = self.label_map[real_idx]\n",
    "        return img, label\n",
    "\n",
    "def evaluate_semi_supervised(method='typiclust', budget=10,\n",
    "                             total_iterations=40000, pseudo_iterations=10000):\n",
    "    \"\"\"\n",
    "    Evaluate semi-supervised learning using a simplified pseudo-labeling approach\n",
    "    \n",
    "    This version uses 400k total iterations on the labeled data\n",
    "    then 100k for fine-tuning on the combined set.\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Load datasets\n",
    "    train_dataset, test_dataset = get_cifar10_datasets()\n",
    "    total_indices = list(range(len(train_dataset)))\n",
    "\n",
    "    # 2. Load a pretrained SimCLR encoder for sample selection\n",
    "    encoder = SimCLRResNet18(feature_dim=128).to(DEVICE)\n",
    "    checkpoint_path = 'model/simclr_cifar_10.pth.tar'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "        encoder.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Loaded pretrained SimCLR model for TPC-RP selection.\")\n",
    "    else:\n",
    "        print(\"Pretrained checkpoint not found; selection may be random.\")\n",
    "    encoder.eval()\n",
    "\n",
    "    # 3. Select labeled samples using Typiclust \n",
    "    labeled_indices = select_samples_typiclust(train_dataset, budget, encoder)\n",
    "    unlabeled_indices = list(set(total_indices) - set(labeled_indices))\n",
    "    print(f\"Labeled samples: {len(labeled_indices)}; Unlabeled samples: {len(unlabeled_indices)}\")\n",
    "\n",
    "    # 4. Stage 1: Train on labeled data (400k iterations)\n",
    "    model = build_wide_resnet()\n",
    "    labeled_subset = Subset(train_dataset, labeled_indices)\n",
    "    labeled_loader = DataLoader(labeled_subset, batch_size=64, shuffle=True)\n",
    "    print(f\"Stage 1: Training on labeled data (400k iters)...\")\n",
    "    model = train_model_iterations(model, labeled_loader, total_iterations, lr=0.03, device=DEVICE)\n",
    "\n",
    "    # 5. Generate pseudo-labels for unlabeled data\n",
    "    unlabeled_subset = Subset(train_dataset, unlabeled_indices)\n",
    "    print(\"Stage 2: Generating pseudo-labels for unlabeled data...\")\n",
    "    pseudo_labels = generate_pseudo_labels(model, unlabeled_subset, batch_size=64, device=DEVICE)\n",
    "\n",
    "    # 6. Combine labeled + pseudo-labeled data, then fine-tune (100k iterations)\n",
    "    combined_dataset = CombinedDataset(train_dataset, labeled_indices, unlabeled_indices, pseudo_labels)\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "    print(f\"Stage 3: Fine-tuning on combined data (100k iters)...\")\n",
    "    model = train_model_iterations(model, combined_loader, pseudo_iterations, lr=0.01, device=DEVICE)\n",
    "\n",
    "    # 7. Final evaluation on test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    acc = evaluate_model(model, test_loader, DEVICE)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def main_experiments(num_experiments=3):\n",
    "    \"\"\"\n",
    "    Run the entire semi-supervised evaluation 3 times with different random seeds,\n",
    "    each time for 400k + 100k iterations. Print out the average accuracy.\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for seed in [11, 22, 33][:num_experiments]:\n",
    "        set_seed(seed)\n",
    "        acc = evaluate_semi_supervised(\n",
    "            method='typiclust',\n",
    "            budget=10,              # 10 labeled samples for CIFAR-10\n",
    "            total_iterations=400000, \n",
    "            pseudo_iterations=100000 \n",
    "        )\n",
    "        accuracies.append(acc)\n",
    "        print(f\"Experiment with seed {seed} => Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    print(f\"\\nAfter {num_experiments} runs:\")\n",
    "    print(f\"Mean Accuracy: {mean_acc*100:.2f}% (+/- {std_acc*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_experiments(num_experiments=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_coursework2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
